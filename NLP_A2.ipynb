{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "with open('./brown_100.txt', 'r') as file_:\n",
    "    corpus = file_.read()\n",
    "\n",
    "def write_freqs(list_: list, filename: str) -> None:\n",
    "    with open(f'./results/{filename}.txt', 'w') as file_:\n",
    "        file_.write('\\n'.join([' '.join(x[0]) + ' | ' + str(x[1]) for x in list_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Language Modeling\n",
    "In the second assignment, you will implement an **n-gram language model** that processes input text to generate, analyze, and calculate the probabilities of sequences of `n` words (n-grams). This model will tokenize text, create n-grams, and compute their frequencies and probabilities, returning the most frequent n-grams in the text.\n",
    "\n",
    "1. **Tokenization**: Convert the input text into a list of words (tokens).\n",
    "2. **N-gram Generation**: Create sequences of `n` tokens from the text, treating each sentence independently with the inclusion of start (`<s>`) and end (`</s>`) markers.\n",
    "3. **Frequency Counting**: Count how many times each n-gram appears in the text.\n",
    "4. **Probability Calculation**: Compute the probabilities of each n-gram based on its frequency and add alpha smoothing to improve generalization of items.\n",
    "5. **Most Frequent N-grams**: Return the most frequent n-grams along with their probabilities, helping to identify common patterns in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place . </s> \\n<s> The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted . </s> \\n<s> The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. . </s> \\n<s> `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' . </s> \\n<s> The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' . </s> \\n<s> It recommended that Fulton legislators act `` to have these laws studied and revised to the end of modernizing and improving them '' . </s> \\n<s> The grand jury commented on a number of other topics , among them the Atlanta and Fulton County purchasing departments which it said `` are well operated and follow generally accepted practices which inure to the best interest of both governments '' . </s> \\n<s> Merger proposed </s> \\n<s> However , the jury said it believes `` these two offices should be combined to achieve greater efficiency and reduce the cost of administration '' . </s> \\n<s> The City Purchasing Department , the jury said , `` is lacking in experienced clerical personnel as a result of city personnel policies '' . </s> \\n<s> It urged that the city `` take steps to remedy '' this problem . </s> \\n<s> Implementation of Georgia's automobile title law was also recommended by the outgoing jury . </s> \\n<s> It urged that the next Legislature `` provide enabling funds and re-set the effective date so that an orderly implementation of the law may be effected '' . </s> \\n<s> The grand jury took a swipe at the State Welfare Department's handling of federal funds granted for child welfare services in foster homes . </s> \\n<s> `` This is one of the major items in the Fulton County general assistance program '' , the jury said , but the State Welfare Department `` has seen fit to distribute these funds through the welfare departments of all the counties in the state with the exception of Fulton County , which receives none of this money . </s> \\n<s> The jurors said they realize `` a proportionate distribution of these funds might disable this program in our less populous counties '' . </s> \\n<s> Nevertheless , `` we feel that in the future Fulton County should receive some portion of these available funds '' , the jurors said . </s> \\n<s> `` Failure to do this will continue to place a disproportionate burden '' on Fulton taxpayers . </s> \\n<s> The jury also commented on the Fulton ordinary's court which has been under fire for its practices in the appointment of appraisers , guardians and administrators and the awarding of fees and compensation . </s> \\n<s> Wards protected </s> \\n<s> The jury said it found the court `` has incorporated into its operating procedures the recommendations '' of two previous grand juries , the Atlanta Bar Association and an interim citizens committee . </s> \\n<s> `` These actions should serve to protect in fact and in effect the court's wards from undue costs and its appointed and elected servants from unmeritorious criticisms '' , the jury said . </s> \\n<s> Regarding Atlanta's new multi-million-dollar airport , the jury recommended `` that when the new management takes charge Jan. 1 the airport be operated in a manner that will eliminate political influences '' . </s> \\n<s> The jury did not elaborate , but it added that `` there should be periodic surveillance of the pricing practices of the concessionaires for the purpose of keeping the prices reasonable '' . </s> \\n<s> Ask jail deputies </s> \\n<s> On other matters , the jury recommended that : ( 1 ) </s> \\n<s> Four additional deputies be employed at the Fulton County Jail and `` a doctor , medical intern or extern be employed for night and weekend duty at the jail '' . </s> \\n<s> ( 2 ) </s> \\n<s> Fulton legislators `` work with city officials to pass enabling legislation that will permit the establishment of a fair and equitable '' pension plan for city employes . </s> \\n<s> The jury praised the administration and operation of the Atlanta Police Department , the Fulton Tax Commissioner's Office , the Bellwood and Alpharetta prison farms , Grady Hospital and the Fulton Health Department . </s> \\n<s> Mayor William B. Hartsfield filed suit for divorce from his wife , Pearl Williams Hartsfield , in Fulton Superior Court Friday . </s> \\n<s> His petition charged mental cruelty . </s> \\n<s> The couple was married Aug. 2 , 1913 . </s> \\n<s> They have a son , William Berry Jr. , and a daughter , Mrs. J. M. Cheshire of Griffin . </s> \\n<s> Attorneys for the mayor said that an amicable property settlement has been agreed upon . </s> \\n<s> The petition listed the mayor's occupation as `` attorney '' and his age as 71 . </s> \\n<s> It listed his wife's age as 74 and place of birth as Opelika , Ala. . </s> \\n<s> The petition said that the couple has not lived together as man and wife for more than a year . </s> \\n<s> The Hartsfield home is at 637 E. Pelham Rd. Aj . </s> \\n<s> Henry L. Bowden was listed on the petition as the mayor's attorney . </s> \\n<s> Hartsfield has been mayor of Atlanta , with exception of one brief interlude , since 1937 . </s> \\n<s> His political career goes back to his election to city council in 1923 . </s> \\n<s> The mayor's present term of office expires Jan. 1 . </s> \\n<s> He will be succeeded by Ivan Allen Jr. , who became a candidate in the Sept. 13 primary after Mayor Hartsfield announced that he would not run for reelection . </s> \\n<s> Georgia Republicans are getting strong encouragement to enter a candidate in the 1962 governor's race , a top official said Wednesday . </s> \\n<s> Robert Snodgrass , state GOP chairman , said a meeting held Tuesday night in Blue Ridge brought enthusiastic responses from the audience . </s> \\n<s> State Party Chairman James W. Dorsey added that enthusiasm was picking up for a state rally to be held Sept. 8 in Savannah at which newly elected Texas Sen. John Tower will be the featured speaker . </s> \\n<s> In the Blue Ridge meeting , the audience was warned that entering a candidate for governor would force it to take petitions out into voting precincts to obtain the signatures of registered voters . </s> \\n<s> Despite the warning , there was a unanimous vote to enter a candidate , according to Republicans who attended . </s> \\n<s> When the crowd was asked whether it wanted to wait one more term to make the race , it voted no -- and there were no dissents . </s> \\n<s> The largest hurdle the Republicans would have to face is a state law which says that before making a first race , one of two alternative courses must be taken : 1 </s> \\n<s> Five per cent of the voters in each county must sign petitions requesting that the Republicans be allowed to place names of candidates on the general election ballot , or 2 </s> \\n<s> The Republicans must hold a primary under the county unit system -- a system which the party opposes in its platform . </s> \\n<s> Sam Caldwell , State Highway Department public relations director , resigned Tuesday to work for Lt. Gov. Garland Byrd's campaign . </s> \\n<s> Caldwell's resignation had been expected for some time . </s> \\n<s> He will be succeeded by Rob Ledford of Gainesville , who has been an assistant more than three years . </s> \\n<s> When the gubernatorial campaign starts , Caldwell is expected to become a campaign coordinator for Byrd . </s> \\n<s> The Georgia Legislature will wind up its 1961 session Monday and head for home -- where some of the highway bond money it approved will follow shortly . </s> \\n<s> Before adjournment Monday afternoon , the Senate is expected to approve a study of the number of legislators allotted to rural and urban areas to determine what adjustments should be made . </s> \\n<s> Gov. Vandiver is expected to make the traditional visit to both chambers as they work toward adjournment . </s> \\n<s> Vandiver likely will mention the $100 million highway bond issue approved earlier in the session as his first priority item . </s> \\n<s> Construction bonds </s> \\n<s> Meanwhile , it was learned the State Highway Department is very near being ready to issue the first $30 million worth of highway reconstruction bonds . </s> \\n<s> The bond issue will go to the state courts for a friendly test suit to test the validity of the act , and then the sales will begin and contracts let for repair work on some of Georgia's most heavily traveled highways . </s> \\n<s> A Highway Department source said there also is a plan there to issue some $3 million to $4 million worth of Rural Roads Authority bonds for rural road construction work . </s> \\n<s> A revolving fund </s> \\n<s> The department apparently intends to make the Rural Roads Authority a revolving fund under which new bonds would be issued every time a portion of the old ones are paid off by tax authorities . </s> \\n<s> Vandiver opened his race for governor in 1958 with a battle in the Legislature against the issuance of $50 million worth of additional rural roads bonds proposed by then Gov. Marvin Griffin . </s> \\n<s> The Highway Department source told The Constitution , however , that Vandiver has not been consulted yet about the plans to issue the new rural roads bonds . </s> \\n<s> Schley County Rep. B. D. Pelham will offer a resolution Monday in the House to rescind the body's action of Friday in voting itself a $10 per day increase in expense allowances . </s> \\n<s> Pelham said Sunday night there was research being done on whether the `` quickie '' vote on the increase can be repealed outright or whether notice would have to first be given that reconsideration of the action would be sought . </s> \\n<s> While emphasizing that technical details were not fully worked out , Pelham said his resolution would seek to set aside the privilege resolution which the House voted through 87-31 . </s> \\n<s> A similar resolution passed in the Senate by a vote of 29-5 . </s> \\n<s> As of Sunday night , there was no word of a resolution being offered there to rescind the action . </s> \\n<s> Pelham pointed out that Georgia voters last November rejected a constitutional amendment to allow legislators to vote on pay raises for future Legislature sessions . </s> \\n<s> A veteran Jackson County legislator will ask the Georgia House Monday to back federal aid to education , something it has consistently opposed in the past . </s> \\n<s> Rep. Mac Barber of Commerce is asking the House in a privilege resolution to `` endorse increased federal support for public education , provided that such funds be received and expended '' as state funds . </s> \\n<s> Barber , who is in his 13th year as a legislator , said there `` are some members of our congressional delegation in Washington who would like to see it ( the resolution ) passed '' . </s> \\n<s> But he added that none of Georgia's congressmen specifically asked him to offer the resolution . </s> \\n<s> The resolution , which Barber tossed into the House hopper Friday , will be formally read Monday . </s> \\n<s> It says that `` in the event Congress does provide this increase in federal funds '' , the State Board of Education should be directed to `` give priority '' to teacher pay raises . </s> \\n<s> Colquitt </s> \\n<s> -- After a long , hot controversy , Miller County has a new school superintendent , elected , as a policeman put it , in the `` coolest election I ever saw in this county '' . </s> \\n<s> The new school superintendent is Harry Davis , a veteran agriculture teacher , who defeated Felix Bush , a school principal and chairman of the Miller County Democratic Executive Committee . </s> \\n<s> Davis received 1,119 votes in Saturday's election , and Bush got 402 . </s> \\n<s> Ordinary Carey Williams , armed with a pistol , stood by at the polls to insure order . </s> \\n<s> `` This was the coolest , calmest election I ever saw '' , Colquitt Policeman Tom Williams said . </s> \\n<s> `` Being at the polls was just like being at church . </s> \\n<s> I didn't smell a drop of liquor , and we didn't have a bit of trouble '' . </s> \\n<s> The campaign leading to the election was not so quiet , however . </s> \\n<s> It was marked by controversy , anonymous midnight phone calls and veiled threats of violence . </s> \\n<s> The former county school superintendent , George P. Callan , shot himself to death March 18 , four days after he resigned his post in a dispute with the county school board . </s> \\n<s> During the election campaign , both candidates , Davis and Bush , reportedly received anonymous telephone calls . </s> \\n<s> Ordinary Williams said he , too , was subjected to anonymous calls soon after he scheduled the election . </s> \\n<s> Many local citizens feared that there would be irregularities at the polls , and Williams got himself a permit to carry a gun and promised an orderly election . </s> \\n<s> Sheriff Felix Tabb said the ordinary apparently made good his promise . </s> \\n<s> `` Everything went real smooth '' , the sheriff said . </s> \\n<s> `` There wasn't a bit of trouble '' . </s> \\n<s> Austin , Texas </s> \\n<s> -- Committee approval of Gov. Price Daniel's `` abandoned property '' act seemed certain Thursday despite the adamant protests of Texas bankers . </s> \\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict # You may import more from collections if needed\n",
    "\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, text, n, alpha=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the NGramModel with text and the value of n.\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.n = n\n",
    "        self.alpha = alpha  # Alpha value for additive smoothing\n",
    "        self.ngrams = {}\n",
    "        self.probabilities = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "    def tokenize(self) -> None:\n",
    "        \"\"\"\n",
    "        Tokenize the text into words. \n",
    "        Fill in the code to split the text into a list of words.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def generate_ngrams(self, tokens: list) -> dict:\n",
    "        \"\"\"\n",
    "        Generate n-grams from the list of tokens.\n",
    "        Fill in the code to create n-grams.\n",
    "        Make sure to treat each sentence independently, include the <s> and </s> tokens.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "        return self.ngrams\n",
    "\n",
    "    def count_frequencies(self) -> None:\n",
    "        \"\"\"\n",
    "        Count the frequencies of each n-gram.\n",
    "        Fill in the code to count n-gram occurrences.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def calculate_probabilities(self) -> None:\n",
    "        \"\"\"\n",
    "        Calculate probabilities of each n-gram based on its frequency. Add alpha smoothing separately.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def most_frequent_ngrams(self, top_n: int = 10) -> list:\n",
    "        \"\"\"\n",
    "        Return the most frequent n-grams and their probabilities.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "    \n",
    "        return sorted_grams\n",
    "\n",
    "# Testing the model with an example\n",
    "text = \"This is a simple example to demonstrate how n-grams work in this assignment.\" #Faster than running the full corpus\n",
    "n = 2  # You can change this to 1, 2, or 3 for uni, bi, or trigrams\n",
    "model = NGramModel(text, n)\n",
    "\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "model.count_frequencies()\n",
    "model.calculate_probabilities()\n",
    "\n",
    "# Print the most frequent n-grams\n",
    "print(model.most_frequent_ngrams(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams\n",
    "\n",
    "In this section, we apply the `NGramModel` to our corpus. The model can be configured to generate unigrams, bigrams, or trigrams by adjusting the value of `n`. After tokenizing the text and generating the n-grams, the model counts the occurrences of each n-gram and calculates their probabilities. The most frequent n-grams are then written to an output file.\n",
    "\n",
    "By adjusting the n-gram size, we can analyze different levels of word dependencies in the text, providing insights into common word sequences and patterns.  \n",
    "\n",
    "The code below will generate unigrams when the NGramModel is implemented correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1  # You can change this to 1, 2, or 3 for uni, bi, or trigrams\n",
    "model = NGramModel(corpus, n)\n",
    "\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "model.count_frequencies()\n",
    "model.calculate_probabilities()\n",
    "\n",
    "write_freqs(model.most_frequent_ngrams(10), 'unigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams\n",
    "Bigrams allow us to explore word pairs and their relationships, providing insight into common word combinations and phrase structures within the corpus.  \n",
    "The code below will generate bigrams when the NGramModel is implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2  # You can change this to 1, 2, or 3 for uni, bi, or trigrams\n",
    "model = NGramModel(corpus, n)\n",
    "\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "model.count_frequencies()\n",
    "model.calculate_probabilities()\n",
    "\n",
    "write_freqs(model.most_frequent_ngrams(10), 'bigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams\n",
    "Trigrams capture three-word sequences, providing a deeper understanding of longer word dependencies and commonly occurring phrases within the text.  \n",
    "The code below will generate trigrams when the NGramModel is implemented correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3  # You can change this to 1, 2, or 3 for uni, bi, or trigrams\n",
    "model = NGramModel(corpus, n)\n",
    "\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "model.count_frequencies()\n",
    "model.calculate_probabilities()\n",
    "\n",
    "write_freqs(model.most_frequent_ngrams(10), 'trigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "Below, we generate **bigrams** with **smoothing** applied by setting `n = 2` and including a smoothing parameter `alpha = 1.0`. Smoothing is useful in language models to handle unseen n-grams by assigning a small probability to them. The model tokenizes the text, generates bigrams, counts their frequencies, and calculates smoothed probabilities. The most frequent bigrams are then written to an output file.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2  # You can change this to 1, 2, or 3 for uni, bi, or trigrams\n",
    "model = NGramModel(corpus, n, alpha=1.0)\n",
    "\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "model.count_frequencies()\n",
    "model.calculate_probabilities()\n",
    "\n",
    "write_freqs(model.most_frequent_ngrams(10), 'bigrams_smoothed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text\n",
    "In this example, we generate text using the **bigrams** model by providing a starting **prompt** (e.g., \"the jury\"). The `generate_text` function creates a sequence of words based on the trained bigram model, starting from the provided prompt and continuing based on the probabilities of the next words.\n",
    "\n",
    "After tokenizing the corpus, generating bigrams, and calculating their frequencies and probabilities, the model uses these learned probabilities to generate text. The generated output is then written to a file.\n",
    "\n",
    "Text generation with n-grams helps illustrate how language models can predict word sequences, allowing us to create new sentences that follow similar patterns found in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model: NGramModel, n: int, prompt: str) -> str:\n",
    "    raise NotImplementedError\n",
    "    return ' '.join(generated_tokens)\n",
    "\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "model.count_frequencies()\n",
    "model.calculate_probabilities()\n",
    "generated_text = generate_text(model, 2, 'the jury')\n",
    "with open(f'./results/generated_bigrams.txt', 'w') as file_:\n",
    "    file_.write(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "Based on the Stanford course: https://web.stanford.edu/class/cs224n/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count based W2V\n",
    "In this section, we use the previously trained **n-gram model** to calculate a **co-occurrence matrix**. A co-occurrence matrix shows how often words appear together within a specific context (in this case, bigrams), providing insights into word relationships in the corpus.\n",
    "\n",
    "Steps:\n",
    "1. **Tokenization and N-gram Generation**: We first tokenize the text and generate bigrams using the `NGramModel`.\n",
    "2. **Vocabulary and Indexing**: The vocabulary is created from the unique tokens, and a word-to-index mapping is established.\n",
    "3. **Co-occurrence Matrix**: Using the generated n-grams, we compute the co-occurrence matrix, where each entry represents how often two words occur together within the bigrams.\n",
    "\n",
    "The co-occurrence matrix provides a numerical representation of word relationships, which is useful for tasks like word embeddings, semantic analysis, and understanding word associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we already have a working ngram model, for the next part we can use the ngrams to calculate the co-occurences.\n",
    "model = NGramModel(corpus, 2)\n",
    "tokens = model.tokenize()\n",
    "ngrams = model.generate_ngrams(tokens)\n",
    "\n",
    "def create_co_matrix(ngrams: list, vocab_size: int, word_to_index: dict) -> np.ndarray:\n",
    "    raise NotImplementedError\n",
    "    return co_matrix\n",
    "\n",
    "vocab = list(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create a word-to-index mapping\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Generate the co-occurrence matrix\n",
    "co_matrix = create_co_matrix(ngrams, vocab_size, word_to_index)\n",
    "\n",
    "# Optionally, print the matrix or parts of it\n",
    "print(co_matrix.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code below you can verify if the co-occurence works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_co_occurrence(word1, word2, co_matrix, word_to_index):\n",
    "    \"\"\"\n",
    "    Print the co-occurrence count for a specific word pair.\n",
    "    \"\"\"\n",
    "    if word1 not in word_to_index or word2 not in word_to_index:\n",
    "        print(f\"One or both words '{word1}' and '{word2}' are not in the vocabulary.\")\n",
    "        return\n",
    "\n",
    "    index1 = word_to_index[word1]\n",
    "    index2 = word_to_index[word2]\n",
    "\n",
    "    co_count = co_matrix[index1][index2]\n",
    "    print(f\"Co-occurrence count for '{word1}' and '{word2}': {co_count}\")\n",
    "\n",
    "check_co_occurrence('the', 'jury', co_matrix, word_to_index)\n",
    "check_co_occurrence('the', 'cat', co_matrix, word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we reduce the dimensionality of the **co-occurrence matrix** using **Singular Value Decomposition (SVD)**. This technique helps in extracting the most important features from the matrix, making it easier to visualize and interpret the relationships between words.\n",
    "\n",
    "Steps:\n",
    "1. **Singular Value Decomposition (SVD)**: The co-occurrence matrix is decomposed into three matrices: \\( M = U \\cdot \\Sigma \\cdot V^T \\).\n",
    "2. **Dimensionality Reduction**: We retain only the top `k` components (default is 2) from the SVD, effectively reducing the dimensionality of the matrix.\n",
    "3. **Reduced Matrix**: The reduced matrix retains the most important semantic information and can be used for tasks such as **word embedding visualization** or **semantic similarity analysis**.\n",
    "\n",
    "Dimensionality reduction allows us to capture the essence of word relationships in fewer dimensions, making it more efficient to process and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    raise NotImplementedError\n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final part, we visualize the reduced word embeddings in **2D space**. After reducing the dimensionality of the co-occurrence matrix using SVD, we plot the embeddings for each word to observe their relationships in a low-dimensional space.\n",
    "\n",
    "Steps:\n",
    "1. **Plotting the Embeddings**: Each word from the vocabulary is plotted based on its 2D coordinates from the reduced matrix.\n",
    "2. **Visualization**: The scatter plot allows us to see how words cluster together or relate to each other based on co-occurrences in the text. Words with similar contexts should appear closer to each other in the plot.\n",
    "3. **Interpretation**: By looking at the 2D plot, we can analyze word similarities, relationships, and clusters formed by words that often occur together.\n",
    "\n",
    "This visualization is a useful tool for understanding the structure of word embeddings and the relationships between words in the corpus.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings():\n",
    "    raise NotImplementedError\n",
    "\n",
    "words = ['movie', 'book', 'mysterious', 'story', 'fascinating', 'good', 'interesting', 'large', 'massive', 'huge']\n",
    "\n",
    "plot_embeddings(M_reduced, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction-Based Word Vectors\n",
    "In this section, we load pre-trained word vectors using the **GloVe** embeddings from the [Stanford NLP paper on GloVe](https://nlp.stanford.edu/pubs/glove.pdf). The pre-trained embeddings provide rich semantic information about words based on large text corpora, allowing us to represent words as vectors that capture their meaning and relationships.\n",
    "\n",
    "Steps:\n",
    "1. **Load GloVe Word Vectors**: We use the GloVe embeddings from the `glove-wiki-gigaword-200` model, which has 200-dimensional vectors for each word.\n",
    "2. **Vocabulary Matching**: We map the words from our corpus to the GloVe vocabulary and extract the corresponding vectors.\n",
    "3. **Dimensionality Reduction**: After obtaining the word vectors, we reduce the dimensionality to visualize the embeddings in 2D space, allowing us to explore word relationships visually.\n",
    "\n",
    "This is an excellent opportunity to explore how pre-trained embeddings like GloVe can be used to improve downstream NLP tasks by providing richer word representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
    "print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "\n",
    "wv_words = list(wv_from_bin.index_to_key)\n",
    "unique_tokens = set(tokens)\n",
    "word2ind = {}\n",
    "M = []\n",
    "idx = 0\n",
    "print('rendering M based on wv')\n",
    "for w in wv_words:\n",
    "    try:\n",
    "        M.append(wv_from_bin.get_vector(w))\n",
    "        word2ind[w] = idx\n",
    "        idx += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print('rendering M based on unique_tokens')\n",
    "for w in unique_tokens:\n",
    "    if w in wv_words:\n",
    "        continue\n",
    "    try:\n",
    "        M.append(wv_from_bin.get_vector(w))\n",
    "        word2ind[w] = idx\n",
    "        idx += 1\n",
    "    except:\n",
    "        pass\n",
    "M = np.stack(M)\n",
    "M_reduced = reduce_to_k_dim(M, k=2)\n",
    "\n",
    "M_lengths = np.linalg.norm(M_reduced, axis=1)\n",
    "M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare plots\n",
    "What stands out when comparing the two different ways of embedding using the given words?\n",
    "\n",
    "(Use this to help answer the question in your assignment document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(M_reduced_normalized, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words with multiple meanings\n",
    "Polysemes and homonyms are words that have more than one meaning (see [this wiki page](https://en.wikipedia.org/wiki/Polysemy) to learn more about the difference between polysemes and homonyms). Your task is to find a word with at least two different meanings such that the top-10 most similar words (according to cosine similarity) contain related words from both meanings.\n",
    "\n",
    "For example:\n",
    "- \"leaves\" has both the \"go_away\" and \"a_structure_of_a_plant\" meanings in the top 10.\n",
    "- \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\" meanings in the top 10.\n",
    "\n",
    "You will probably need to try several polysemous or homonymic words before you find one.\n",
    "\n",
    "Once you discover a word that fits this criterion, state the word and explain the multiple meanings that occur in the top 10. Reflect on why many of the polysemous or homonymic words you tried didn’t work (i.e., why the top-10 most similar words only contain one of the word's meanings).\n",
    "\n",
    "**Note**: Use the `wv_from_bin.most_similar(word)` function to get the top 10 most similar words. This function ranks all other words in the vocabulary based on their cosine similarity to the given word. For further assistance, you can refer to the [Gensim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms\n",
    "When considering **Cosine Similarity**, it's often useful to think of **Cosine Distance**, which is simply 1 - Cosine Similarity.\n",
    "\n",
    "Find three words \\( (w_1, w_2, w_3) \\) where:\n",
    "- \\(w_1\\) and \\(w_2\\) are **synonyms**.\n",
    "- \\(w_1\\) and \\(w_3\\) are **antonyms**.\n",
    "\n",
    "However, you need to find a case where **Cosine Distance** \\( (w_1, w_3) < \\text{Cosine Distance} (w_1, w_2) \\). In other words, the antonym is **closer** to the word than its synonym in the vector space.\n",
    "\n",
    "For example, \"happy\" (\\(w_1\\)) may be closer to \"sad\" (\\(w_3\\)) than to \"cheerful\" (\\(w_2\\)), which is counter-intuitive since we expect synonyms to be closer than antonyms. \n",
    "\n",
    "Once you find such an example, provide a possible explanation for why this result may have occurred.\n",
    "\n",
    "Use the `wv_from_bin.distance(w1, w2)` function to compute the cosine distance between two words. You can refer to the [Gensim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html) for further assistance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies\n",
    "Word vectors have demonstrated the ability to solve analogies based on their learned semantic relationships. For instance, consider the analogy: \n",
    "\n",
    "**\"man : grandfather :: woman : x\"** \n",
    "\n",
    "(Read: man is to grandfather as woman is to x). Using word vectors, we can find the word \\(x\\) that completes the analogy.\n",
    "\n",
    "In the example provided, we use the `most_similar` function from the Gensim library. This function identifies words that are most similar to the words in the positive list and most dissimilar to those in the negative list. For analogy solving, it effectively computes:\n",
    "\n",
    "\\[ \\text{word}(x) = \\text{most similar to} (\\text{woman} + \\text{grandfather} - \\text{man}) \\]\n",
    "\n",
    "The result is the word with the highest cosine similarity to the target vector. You can use this approach to explore various analogies and gain insights into the semantic structure of word vectors.\n",
    "\n",
    "Refer to the [Gensim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html) for more details on the `most_similar` function and how word vectors handle analogy-solving tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('grandmother', 0.7608445286750793), ('granddaughter', 0.7200808525085449), ('daughter', 0.7168302536010742), ('mother', 0.7151536345481873), ('niece', 0.7005682587623596), ('father', 0.6659887433052063), ('aunt', 0.6623408794403076), ('grandson', 0.6618767976760864), ('grandparents', 0.644661009311676), ('wife', 0.6445354223251343)]\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to answer the analogy -- man : grandfather :: woman : x\n",
    "print(wv_from_bin.most_similar(positive=['woman', 'grandfather'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias in word vectors\n",
    "a. **Observation**: Consider the results of a word vector model when queried with analogies involving \"man\", \"woman\", and \"profession\". What are the top results returned by the model for each query?\n",
    "\n",
    "[Your Answer]\n",
    "\n",
    "b. **Analysis**: Do you observe any gender biases in the results? For example, are certain professions more closely associated with \"man\" or \"woman\" based on the word vectors? Discuss how word embeddings might perpetuate societal biases present in the training data.\n",
    "\n",
    "[Your Answer]\n",
    "\n",
    "c. **Reflection**: How might such biases affect the fairness of machine learning models that use word embeddings? Suggest potential strategies for mitigating gender bias in word vector models.\n",
    "\n",
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv_from_bin.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
    "print()\n",
    "print(wv_from_bin.most_similar(positive=['woman', 'profession'], negative=['man']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
